{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sV2NUYLWm_u9"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Optional) Get datasets from Github"
      ],
      "metadata": {
        "id": "pLlUBzZzRBSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access github repo\n",
        "REPO_URL = \"https://github.com/cchen744/CornYield_NN.git\"\n",
        "REPO_NAME = \"CornYield_NN\" # This is the folder name that will be created\n",
        "\n",
        "# 4. Clone the repository\n",
        "# We use the token for secure, authenticated access\n",
        "!git clone https://github.com/cchen744/CornYield_NN.git\n",
        "\n",
        "# 5. Change the working directory into the cloned repository folder\n",
        "import os\n",
        "os.chdir(REPO_NAME)\n",
        "\n",
        "# Verify the files are there (you should see your notebook and dataset files)\n",
        "print(f\"Current directory contents in /{REPO_NAME}:\")\n",
        "!ls -F"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_hJMTKeRE5h",
        "outputId": "98fa2555-c41e-4197-aa4a-9be82e4f66a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CornYield_NN'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 52 (delta 23), reused 26 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (52/52), 3.17 MiB | 6.43 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "Current directory contents in /CornYield_NN:\n",
            "counties.csv\t\t     prism_weather_1995-2009.csv  weather_clean.csv\n",
            "final_dataset.csv\t     prism_weather_2010-2024.csv  wide_data.csv\n",
            "get_data.ipynb\t\t     prism_weather.csv\t\t  yield_clean.csv\n",
            "LSTM_+_MLP_model.ipynb\t     RandomForest.ipynb\t\t  yield.csv\n",
            "nasa_weather.csv\t     soil_clean.csv\n",
            "prism_weather_1984-1994.csv  soil_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset preparation"
      ],
      "metadata": {
        "id": "LtRq2domcRyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 3 types of datasets:\n",
        "1. yield data, which looks like: <br>\n",
        "Year, County, Yield\n",
        "2. monthly weather data, which looks like: <br>\n",
        "County, Year, Month, solar_radiation, humidity, ...., vpd_max\n",
        "3. soil dataset: <br>\n",
        "County, bdod, cec, ...."
      ],
      "metadata": {
        "id": "nqu244tXaGeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load 3 datasets\n",
        "yield_df_path = \"/content/CornYield_NN/yield_clean.csv\"\n",
        "soil_df_path = \"/content/CornYield_NN/soil_clean.csv\"\n",
        "weather_df_path = \"/content/CornYield_NN/weather_clean.csv\"\n",
        "yield_df = pd.read_csv(yield_df_path)\n",
        "soil_df = pd.read_csv(soil_df_path)\n",
        "weather_df = pd.read_csv(weather_df_path)\n",
        "print(yield_df.head())\n",
        "print(soil_df.head())\n",
        "print(weather_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKZgYQXIbNla",
        "outputId": "01ab08af-a017-4b6e-8a0e-4d9aa1f28b4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Year      County  Yield\n",
            "0  2024       adams  120.1\n",
            "1  2024  green lake  168.3\n",
            "2  2024      juneau  141.0\n",
            "3  2024   marquette  126.0\n",
            "4  2024     portage  159.5\n",
            "     County  bdod  cec  clay  nitrogen  phh2o  sand  silt  soc\n",
            "0     adams   132  163   192       698     60   489   319  375\n",
            "1   ashland   115  249   258       601     51   216   526  605\n",
            "2    barron   133  150   185       516     55   317   498  677\n",
            "3  bayfield   111  266   187       545     50   480   333  668\n",
            "4     brown   134  289   308       781     66   328   364  429\n",
            "  County  Year  Month  solar_radiation  humidity  wind_speed  wind_speed_max  \\\n",
            "0  adams  1984      4            16.85     75.80        4.40           12.36   \n",
            "1  adams  1984      5            17.63     74.25        3.26            9.64   \n",
            "2  adams  1984      6            20.75     78.96        3.42            9.15   \n",
            "3  adams  1984      7            21.90     74.48        2.68            8.19   \n",
            "4  adams  1984      8            18.52     68.76        2.31            6.40   \n",
            "\n",
            "   precip  temp_min  temp_mean  temp_max  dewpoint_mean  vpd_min  vpd_max  \n",
            "0    3.83      35.5       46.5      57.5           29.7     1.57    10.85  \n",
            "1    2.27      41.4       53.7      65.9           40.2     1.12    13.40  \n",
            "2    5.96      56.2       68.1      80.0           57.0     1.20    18.43  \n",
            "3    3.29      56.9       69.4      81.9           58.0     1.08    20.90  \n",
            "4    2.54      58.5       70.9      83.2           60.5     0.85    20.17  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the dataset put into LSTM has the format:<br>\n",
        "(batch_size,num_months,num_features)"
      ],
      "metadata": {
        "id": "FT7Z3z3yNnax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare monthly sequences\n",
        "\n",
        "# Select monthly columns for LSTM\n",
        "monthly_features = weather_df.columns[3:]\n",
        "# Sort and group monthly dataset by (County, Year)\n",
        "grouped_weather = (weather_df\n",
        "    .sort_values([\"County\", \"Year\", \"Month\"])\n",
        "    .groupby([\"County\", \"Year\"])\n",
        ")\n",
        "\n",
        "# Build dictionary: key = (county, year), value = monthly sequence array\n",
        "monthly_weather_dict = {}\n",
        "\n",
        "for (county, year), g in grouped_weather:\n",
        "    seq = g[monthly_features].values  # shape = (num_months, num_features)\n",
        "    monthly_weather_dict[(county, year)] = seq\n",
        "\n",
        "# Show an example\n",
        "first_key = list(monthly_weather_dict.keys())[0]\n",
        "print(\"Example key:\", first_key)\n",
        "print(\"Sequence shape:\", monthly_weather_dict[first_key].shape)\n",
        "print(monthly_weather_dict[first_key][:3])  # first 3 months\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9-xh820Nk9e",
        "outputId": "3342faba-46a0-4e39-85c1-bac7e81f0b69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example key: ('adams', np.int64(1984))\n",
            "Sequence shape: (6, 11)\n",
            "[[16.85 75.8   4.4  12.36  3.83 35.5  46.5  57.5  29.7   1.57 10.85]\n",
            " [17.63 74.25  3.26  9.64  2.27 41.4  53.7  65.9  40.2   1.12 13.4 ]\n",
            " [20.75 78.96  3.42  9.15  5.96 56.2  68.1  80.   57.    1.2  18.43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train test split"
      ],
      "metadata": {
        "id": "VeW1_TRUQ3KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Original\n",
        "\"\"\"\n",
        "# Find all (county, year) pairs with yield + weather keys, store keys in a list\n",
        "all_keys = list(monthly_weather_dict.keys())\n",
        "# Perform train_test_split on key_list\n",
        "train_keys, test_keys = train_test_split(all_keys, test_size=0.2, random_state=42)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cbhdJtf6czZF",
        "outputId": "762dab25-48a9-4a99-ace3-43f8e10ff356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Find all (county, year) pairs with yield + weather keys, store keys in a list\\nall_keys = list(monthly_weather_dict.keys())\\n# Perform train_test_split on key_list\\ntrain_keys, test_keys = train_test_split(all_keys, test_size=0.2, random_state=42)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE skip data that is not present in both yield and weather\n",
        "\n",
        "# Get all (county, year) pairs from monthly_weather_dict\n",
        "all_weather_keys = set(monthly_weather_dict.keys())\n",
        "\n",
        "# Ensure 'Year' column in yield_df is np.int64 to match weather_dict keys\n",
        "yield_df['Year'] = yield_df['Year'].astype(np.int64)\n",
        "\n",
        "# Get all (county, year) pairs from yield_df with consistent Year type\n",
        "all_yield_keys = set(tuple(row) for row in yield_df[['County', 'Year']].values)\n",
        "\n",
        "# Find the intersection of keys (only keep pairs that have both weather and yield data)\n",
        "all_keys = list(all_weather_keys.intersection(all_yield_keys))\n",
        "\n",
        "# Perform train_test_split on key_list\n",
        "train_keys, test_keys = train_test_split(all_keys, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ZoguXGEBTzrs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE fixed issue with incorrect datatypes and assignments\n",
        "# Dataset preparations -- GitHub\n",
        "# Added: dataset normalization\n",
        "class NormalizedYieldDataset(Dataset):\n",
        "    def __init__(self, keys, yield_df, soil_df, weather_dict,\n",
        "                 weather_scaler=None, soil_scaler=None, yield_scaler=None,\n",
        "                 fit_scalers=False):\n",
        "        self.keys = keys\n",
        "        self.yield_df = yield_df\n",
        "        self.soil_df = soil_df\n",
        "        self.weather_dict = weather_dict\n",
        "\n",
        "        # initialize scalers\n",
        "        if fit_scalers:\n",
        "            # Fit weather scaler\n",
        "            all_weather = np.vstack([weather_dict[k] for k in keys])\n",
        "            self.weather_scaler = StandardScaler()\n",
        "            self.weather_scaler.fit(all_weather)\n",
        "\n",
        "            # Fit soil scaler\n",
        "            soil_counties = [k[0] for k in keys]\n",
        "            soil_data = soil_df[soil_df['County'].isin(soil_counties)].drop('County', axis=1).values\n",
        "            self.soil_scaler = StandardScaler()\n",
        "            self.soil_scaler.fit(soil_data)\n",
        "\n",
        "            # Fit yield scaler\n",
        "            yield_data = []\n",
        "            for county, year in keys:\n",
        "                y = yield_df[(yield_df[\"County\"]==county) & (yield_df[\"Year\"]==year)][\"Yield\"].values[0]\n",
        "                yield_data.append(y)\n",
        "            self.yield_scaler = StandardScaler()\n",
        "            self.yield_scaler.fit(np.array(yield_data).reshape(-1, 1))\n",
        "        else:\n",
        "            self.weather_scaler = weather_scaler\n",
        "            self.soil_scaler = soil_scaler\n",
        "            self.yield_scaler = yield_scaler\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        county, year = self.keys[idx]\n",
        "\n",
        "        # Get and normalize weather\n",
        "        weather_seq = self.weather_dict[(county, year)]\n",
        "        weather_seq = self.weather_scaler.transform(weather_seq)\n",
        "        weather_seq = torch.tensor(weather_seq, dtype=torch.float32)\n",
        "\n",
        "        # Get and normalize yield\n",
        "        y = self.yield_df[\n",
        "            (self.yield_df[\"County\"]==county) &\n",
        "            (self.yield_df[\"Year\"]==year)][\"Yield\"].values[0]\n",
        "        y_normalized = self.yield_scaler.transform([[y]])[0]  # Shape: (1,)\n",
        "\n",
        "        # Get and normalize soil\n",
        "        soil_row = self.soil_df[self.soil_df[\"County\"] == county].iloc[0]\n",
        "        soil_vec = soil_row.drop(\"County\").values.astype(np.float32)\n",
        "        soil_vec = self.soil_scaler.transform([soil_vec])[0]\n",
        "        soil_vec = torch.tensor(soil_vec, dtype=torch.float32)\n",
        "\n",
        "        return weather_seq, soil_vec, torch.tensor([y_normalized[0]], dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "Sgff0xoSev3K"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "batch_size = 32\n",
        "# create datasets\n",
        "train_dataset = NormalizedYieldDataset(\n",
        "    train_keys, yield_df, soil_df, monthly_weather_dict,\n",
        "    fit_scalers=True\n",
        ")\n",
        "\n",
        "test_dataset = NormalizedYieldDataset(\n",
        "    test_keys, yield_df, soil_df, monthly_weather_dict,\n",
        "    weather_scaler=train_dataset.weather_scaler,\n",
        "    soil_scaler=train_dataset.soil_scaler,\n",
        "    yield_scaler=train_dataset.yield_scaler,\n",
        "    fit_scalers=False\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "I8VecCmLZYfm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "# Model assumes that there is at least 1 dimension in monthly, and static\n",
        "class YieldLSTMMLPConnected(nn.Module):\n",
        "    def __init__(self,\n",
        "                    monthly_dim=11,     # Avg's by month (seq features, should be 11 features from weather_clean.csv)\n",
        "                    monthly_layers=2,\n",
        "                    monthly_hidden=128,\n",
        "                    static_dim=8,    # number of static features (should be 8 features from soil data)\n",
        "                    static_hidden=64,\n",
        "                    head_hidden=128,\n",
        "                    output_dim=1,\n",
        "                    dropout=0.2\n",
        "                    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Monthly branch LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=monthly_dim,\n",
        "            hidden_size=monthly_hidden,\n",
        "            num_layers=monthly_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=False\n",
        "        )\n",
        "\n",
        "        # UPDATE BatchNorm for LSTM output\n",
        "        self.monthly_bn = nn.BatchNorm1d(monthly_hidden)\n",
        "        self.monthly_proj = nn.Sequential(\n",
        "            nn.Linear(monthly_hidden, monthly_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # UPDATE Static branch with BatchNorm\n",
        "        self.static_bn = nn.BatchNorm1d(static_dim)\n",
        "        # Static branch MLP\n",
        "        self.static_proj = nn.Sequential(\n",
        "            nn.Linear(static_dim, static_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(static_hidden, static_hidden),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Combined head (output of combined branches)\n",
        "        combined_dim = monthly_hidden + static_hidden\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(combined_dim, head_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(head_hidden, head_hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(head_hidden//2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, weather_seq, static):\n",
        "        feats = []\n",
        "\n",
        "        # Monthly shape (batch, seq_len, monthly_dim)\n",
        "        # LSTM: take last hidden state\n",
        "        lstm_out, (h_n, c_n) = self.lstm(weather_seq)\n",
        "        # h_n shape: (num_layers, batch, hidden)\n",
        "        last_h = h_n[-1] # (batch, monthly_hidden)\n",
        "        # UPDATE batch normalization\n",
        "        last_h = self.monthly_bn(last_h)\n",
        "        monthly_emb = self.monthly_proj(last_h)\n",
        "\n",
        "        # Static branch\n",
        "        static = self.static_bn(static)\n",
        "        static_emb = self.static_proj(static)\n",
        "\n",
        "        feats.append(monthly_emb)\n",
        "        feats.append(static_emb)\n",
        "\n",
        "        combined = torch.cat(feats, dim=1)\n",
        "        out = self.head(combined)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ziySIDb4pdtc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE removed yearly in for loops within train_loader\n",
        "# Training\n",
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=100, # UPDATE num_epoch = 50 --> num_epoch = 100\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-5,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    early_stop_patience=9\n",
        "):\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs+1):\n",
        "        # -------- TRAIN MODE --------\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for monthly, static, target in train_loader: # UPDATE removed yearly\n",
        "            monthly = monthly.to(device)\n",
        "            static = static.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(monthly, static) # UPDATE removed yearly\n",
        "\n",
        "            loss = criterion(preds, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # -------- VAL MODE --------\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for monthly, static, target in val_loader: # UPDATE removed yearly\n",
        "                monthly = monthly.to(device)\n",
        "                static = static.to(device)\n",
        "                target = target.to(device)\n",
        "\n",
        "                preds = model(monthly, static) # UPDATE removed yearly\n",
        "                loss = criterion(preds, target)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        val_loss = np.mean(val_losses)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # ---- EARLY STOP ----\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_yield_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= early_stop_patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "    print(\"Training completed. Best model saved as best_yield_model.pt\")"
      ],
      "metadata": {
        "id": "R2pZ_cYepoKh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Testing\n",
        "model = YieldLSTMMLPConnected()\n",
        "train_model(model, train_loader, test_loader)"
      ],
      "metadata": {
        "id": "pJOJtZjyZW9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2da4a8e-6f03-40b1-cc39-e6b9970cfa0d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train Loss: 0.5887 | Val Loss: 0.6263\n",
            "Epoch 002 | Train Loss: 0.4186 | Val Loss: 0.4121\n",
            "Epoch 003 | Train Loss: 0.3848 | Val Loss: 0.3724\n",
            "Epoch 004 | Train Loss: 0.3613 | Val Loss: 0.3668\n",
            "Epoch 005 | Train Loss: 0.3343 | Val Loss: 0.4282\n",
            "Epoch 006 | Train Loss: 0.2980 | Val Loss: 0.3393\n",
            "Epoch 007 | Train Loss: 0.3106 | Val Loss: 0.3057\n",
            "Epoch 008 | Train Loss: 0.2594 | Val Loss: 0.3027\n",
            "Epoch 009 | Train Loss: 0.2640 | Val Loss: 0.2724\n",
            "Epoch 010 | Train Loss: 0.2421 | Val Loss: 0.3236\n",
            "Epoch 011 | Train Loss: 0.2532 | Val Loss: 0.2509\n",
            "Epoch 012 | Train Loss: 0.2521 | Val Loss: 0.2303\n",
            "Epoch 013 | Train Loss: 0.2301 | Val Loss: 0.2853\n",
            "Epoch 014 | Train Loss: 0.2446 | Val Loss: 0.2370\n",
            "Epoch 015 | Train Loss: 0.2314 | Val Loss: 0.2403\n",
            "Epoch 016 | Train Loss: 0.2365 | Val Loss: 0.2088\n",
            "Epoch 017 | Train Loss: 0.1932 | Val Loss: 0.1956\n",
            "Epoch 018 | Train Loss: 0.2052 | Val Loss: 0.2138\n",
            "Epoch 019 | Train Loss: 0.1959 | Val Loss: 0.1953\n",
            "Epoch 020 | Train Loss: 0.1944 | Val Loss: 0.1759\n",
            "Epoch 021 | Train Loss: 0.2069 | Val Loss: 0.2507\n",
            "Epoch 022 | Train Loss: 0.1937 | Val Loss: 0.1915\n",
            "Epoch 023 | Train Loss: 0.1867 | Val Loss: 0.1991\n",
            "Epoch 024 | Train Loss: 0.1728 | Val Loss: 0.1706\n",
            "Epoch 025 | Train Loss: 0.1681 | Val Loss: 0.1823\n",
            "Epoch 026 | Train Loss: 0.1925 | Val Loss: 0.1680\n",
            "Epoch 027 | Train Loss: 0.2153 | Val Loss: 0.2346\n",
            "Epoch 028 | Train Loss: 0.1739 | Val Loss: 0.1751\n",
            "Epoch 029 | Train Loss: 0.1850 | Val Loss: 0.1660\n",
            "Epoch 030 | Train Loss: 0.1648 | Val Loss: 0.1627\n",
            "Epoch 031 | Train Loss: 0.1625 | Val Loss: 0.1562\n",
            "Epoch 032 | Train Loss: 0.1718 | Val Loss: 0.1761\n",
            "Epoch 033 | Train Loss: 0.1508 | Val Loss: 0.1565\n",
            "Epoch 034 | Train Loss: 0.1544 | Val Loss: 0.1567\n",
            "Epoch 035 | Train Loss: 0.1430 | Val Loss: 0.1774\n",
            "Epoch 036 | Train Loss: 0.1473 | Val Loss: 0.1623\n",
            "Epoch 037 | Train Loss: 0.1340 | Val Loss: 0.1477\n",
            "Epoch 038 | Train Loss: 0.1740 | Val Loss: 0.1512\n",
            "Epoch 039 | Train Loss: 0.1605 | Val Loss: 0.1777\n",
            "Epoch 040 | Train Loss: 0.1560 | Val Loss: 0.1441\n",
            "Epoch 041 | Train Loss: 0.1469 | Val Loss: 0.1541\n",
            "Epoch 042 | Train Loss: 0.1394 | Val Loss: 0.1379\n",
            "Epoch 043 | Train Loss: 0.1344 | Val Loss: 0.1538\n",
            "Epoch 044 | Train Loss: 0.1264 | Val Loss: 0.2187\n",
            "Epoch 045 | Train Loss: 0.1483 | Val Loss: 0.1355\n",
            "Epoch 046 | Train Loss: 0.1475 | Val Loss: 0.1621\n",
            "Epoch 047 | Train Loss: 0.1427 | Val Loss: 0.1869\n",
            "Epoch 048 | Train Loss: 0.1261 | Val Loss: 0.1427\n",
            "Epoch 049 | Train Loss: 0.1475 | Val Loss: 0.1820\n",
            "Epoch 050 | Train Loss: 0.1457 | Val Loss: 0.1588\n",
            "Epoch 051 | Train Loss: 0.1262 | Val Loss: 0.1591\n",
            "Epoch 052 | Train Loss: 0.1295 | Val Loss: 0.1391\n",
            "Epoch 053 | Train Loss: 0.1284 | Val Loss: 0.1534\n",
            "Epoch 054 | Train Loss: 0.1135 | Val Loss: 0.1388\n",
            "Early stopping triggered!\n",
            "Training completed. Best model saved as best_yield_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model\n",
        "best_model = YieldLSTMMLPConnected()\n",
        "best_model.load_state_dict(torch.load(\"best_yield_model.pt\"))"
      ],
      "metadata": {
        "id": "T4oChuLgDbEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc93212-a823-4e1e-e499-2bfc275dbf68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}